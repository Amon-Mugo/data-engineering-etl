# Data Engineering ETL Pipeline (PostgreSQL)

[![Python](https://img.shields.io/badge/Python-3.13-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT)

## ğŸ“Œ Project Overview

This project demonstrates a **robust ETL (Extract, Transform, Load) pipeline** built using **Python**, **Pandas**, and **PostgreSQL**. It simulates a real-world workflow where raw data is extracted from a CSV source, transformed to meet business requirements, and loaded into a relational database for analytics and reporting.

Designed for aspiring data engineers, this project showcases:

* Modular ETL architecture
* Database integration with PostgreSQL
* Environment variable management for security
* Clean, reproducible project structure

---

## ğŸ— Architecture Diagram

```text
CSV File (Source)
     â†“
[ Extract ]  â†’  [ Transform ]  â†’  [ Load ]
     â†“              â†“               â†“
  Pandas         Pandas         PostgreSQL
```

---

## ğŸ“‚ Project Structure

```text
data_engineering/
â”‚â”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py      # Extracts raw data
â”‚   â”œâ”€â”€ transform.py    # Cleans & transforms data
â”‚   â”œâ”€â”€ load.py         # Loads data into PostgreSQL
â”‚   â””â”€â”€ pipeline.py     # Orchestrates ETL steps
â”‚
â”‚â”€â”€ data/
â”‚   â””â”€â”€ employees.csv   # Source dataset
â”‚
â”‚â”€â”€ .env                # Environment variables (ignored in GitHub)
â”‚â”€â”€ requirements.txt    # Python dependencies
â”‚â”€â”€ README.md           # Project documentation
```

---

## ğŸ› ï¸ Technologies Used

* **Python 3.13**
* **Pandas** â€“ data manipulation
* **SQLAlchemy** â€“ database engine/ORM
* **PostgreSQL** â€“ relational database
* **psycopg2** â€“ PostgreSQL driver
* **python-dotenv** â€“ environment variable management
* **Git & GitHub** â€“ version control

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Amon-Mugo/data-engineering-etl.git
cd data-engineering-etl
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment Variables

Create a `.env` file in the project root:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=data_engineering_test
DB_USER=postgres
DB_PASSWORD=YOUR_PASSWORD
```

âš ï¸ `.env` is ignored from GitHub to protect sensitive data.

---

## ğŸš€ Running the ETL Pipeline

From the project root, run:

```bash
python -m etl.pipeline
```

Expected output:

```text
Starting ETL pipeline...
Data extracted
Data transformed
Data loaded into PostgreSQL
Pipeline finished successfully
```

---

## ğŸ—„ï¸ Database Verification

After running the pipeline, connect to PostgreSQL and run:

```sql
SELECT * FROM employees;
```

Expected result:

* Cleaned employee records
* `age` as integer
* `salary` increased by 10%

---

## ğŸ” Security Best Practices

* Database credentials are stored in `.env`
* `.env` and `venv/` are excluded from GitHub via `.gitignore`
* Modular code avoids hardcoding sensitive data

---

## ğŸ“ˆ Skills Demonstrated

* ETL pipeline design and orchestration
* Data cleaning & transformation
* PostgreSQL integration
* Python project structuring
* Environment-based configuration
* Git version control best practices

---

## ğŸ”® Future Improvements

* Add logging instead of print statements
* Implement data validation checks
* Incremental data loading
* Dockerize the pipeline
* Schedule ETL with Airflow or Cron jobs

---

## ğŸ‘¤ Author

**Amon**
Aspiring Data Engineer | Data Science Background

---

## â­ Why This Project Matters

This project reflects **real entry-level data engineering work** and is suitable for:

* Portfolio presentation
* Internship applications
* Junior Data Engineer roles
